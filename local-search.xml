<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LLM 推理计算详解</title>
    <link href="/2025/06/17/llm-inference-computation/"/>
    <url>/2025/06/17/llm-inference-computation/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM-推理计算详解"><a href="#LLM-推理计算详解" class="headerlink" title="LLM 推理计算详解"></a>LLM 推理计算详解</h1><h2 id="一、基础计算单元"><a href="#一、基础计算单元" class="headerlink" title="一、基础计算单元"></a>一、基础计算单元</h2><h3 id="1-注意力机制计算"><a href="#1-注意力机制计算" class="headerlink" title="1. 注意力机制计算"></a>1. 注意力机制计算</h3><p>注意力机制是 Transformer 架构的核心组件，其计算复杂度为 O(n²d)，其中：</p><ul><li>n 是序列长度</li><li>d 是隐藏层维度</li></ul><p>主要计算步骤：</p><ol><li>Query、Key、Value 矩阵计算：O(nd²)</li><li>注意力分数计算：O(n²d)</li><li>注意力权重与 Value 的加权和：O(n²d)</li></ol><h3 id="2-前馈网络计算"><a href="#2-前馈网络计算" class="headerlink" title="2. 前馈网络计算"></a>2. 前馈网络计算</h3><p>前馈网络（FFN）的计算复杂度为 O(nd²)，其中：</p><ul><li>输入层到隐藏层：O(nd²)</li><li>隐藏层到输出层：O(nd²)</li></ul><h2 id="二、计算优化策略"><a href="#二、计算优化策略" class="headerlink" title="二、计算优化策略"></a>二、计算优化策略</h2><h3 id="1-量化计算"><a href="#1-量化计算" class="headerlink" title="1. 量化计算"></a>1. 量化计算</h3><ul><li>INT8 量化：将 FP32 转换为 INT8，减少 75% 的内存占用</li><li>混合精度计算：关键层保持 FP16，其他层使用 INT8</li><li>量化感知训练：在训练时模拟量化效果</li></ul><h3 id="2-并行计算策略"><a href="#2-并行计算策略" class="headerlink" title="2. 并行计算策略"></a>2. 并行计算策略</h3><ol><li><p>张量并行</p><ul><li>将权重矩阵分片到多个设备</li><li>计算时进行通信和聚合</li><li>适用于大型模型</li></ul></li><li><p>流水线并行</p><ul><li>将模型层分配到不同设备</li><li>实现计算和通信重叠</li><li>适用于长序列处理</li></ul></li><li><p>数据并行</p><ul><li>多个设备处理不同批次</li><li>适用于小模型多请求场景</li></ul></li></ol><h3 id="3-内存优化计算"><a href="#3-内存优化计算" class="headerlink" title="3. 内存优化计算"></a>3. 内存优化计算</h3><ol><li><p>激活重计算</p><ul><li>前向传播时不保存中间激活值</li><li>反向传播时重新计算</li><li>内存使用量减少 50%</li></ul></li><li><p>梯度检查点</p><ul><li>选择性保存中间状态</li><li>在需要时重新计算</li><li>平衡内存使用和计算开销</li></ul></li></ol><h2 id="三、实际性能分析"><a href="#三、实际性能分析" class="headerlink" title="三、实际性能分析"></a>三、实际性能分析</h2><h3 id="1-计算瓶颈"><a href="#1-计算瓶颈" class="headerlink" title="1. 计算瓶颈"></a>1. 计算瓶颈</h3><ul><li>注意力机制：占总计算量的 40-60%</li><li>前馈网络：占总计算量的 30-40%</li><li>其他操作：占总计算量的 10-20%</li></ul><h3 id="2-优化效果"><a href="#2-优化效果" class="headerlink" title="2. 优化效果"></a>2. 优化效果</h3><ol><li><p>量化优化</p><ul><li>INT8 量化：推理速度提升 2-3 倍</li><li>混合精度：内存使用减少 50%</li></ul></li><li><p>并行优化</p><ul><li>张量并行：线性扩展效率</li><li>流水线并行：吞吐量提升 2-4 倍</li></ul></li><li><p>内存优化</p><ul><li>激活重计算：内存使用减少 50%</li><li>梯度检查点：内存使用减少 30%</li></ul></li></ol><h2 id="四、未来发展趋势"><a href="#四、未来发展趋势" class="headerlink" title="四、未来发展趋势"></a>四、未来发展趋势</h2><h3 id="1-计算架构创新"><a href="#1-计算架构创新" class="headerlink" title="1. 计算架构创新"></a>1. 计算架构创新</h3><ul><li>专用硬件加速器</li><li>新型注意力机制</li><li>自适应计算策略</li></ul><h3 id="2-算法优化方向"><a href="#2-算法优化方向" class="headerlink" title="2. 算法优化方向"></a>2. 算法优化方向</h3><ul><li>稀疏注意力计算</li><li>动态计算路径</li><li>自适应量化策略</li></ul><h3 id="3-系统级优化"><a href="#3-系统级优化" class="headerlink" title="3. 系统级优化"></a>3. 系统级优化</h3><ul><li>分布式推理框架</li><li>自动并行化</li><li>动态批处理</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Inference</tag>
      
      <tag>Computation</tag>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>notebook</title>
    <link href="/2025/06/05/notebook/"/>
    <url>/2025/06/05/notebook/</url>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h3 id="1-Basics"><a href="#1-Basics" class="headerlink" title="1. Basics"></a>1. Basics</h3><p>Tokenization &amp; Tokenizer：在字符串和整数序列之间进行转换的工具，建立一个映射关系。</p><ul><li>BPE：Byte Pair Encoding，一种常用的分词方法，将字符串分解为子字符串，然后合并最常见的子字符串，直到达到预定的词汇表大小。</li></ul><p>Architecture: Transformer 及其更新</p><ul><li>激活函数：ReLU、SwiGLU  </li><li>位置编码（Positional Encoding）：RoPE、ALiBi</li><li>归一化（Normalization）：Layer Normalization、RMSNorm</li><li>归一化的位置：pre-norm、post-norm</li><li>MLP：Dense、MoE</li><li>注意力机制（Attention Mechanism）：full、sliding window、linear</li><li>Lower-dimension attention： group-query attention(GQA)、multi-head latent attention(MLA)</li></ul><p>Training:</p><ul><li>Optimizer： AdamW</li><li>Learning Rate Scheduler： Cosine、Cosine with Warmup、Linear、Linear with Warmup</li><li>Loss Function： Cross-Entropy、Label Smoothing、KL-Divergence</li><li>Regularization： Dropout、Weight Decay</li></ul><h3 id="2-System"><a href="#2-System" class="headerlink" title="2. System"></a>2. System</h3><p>Kernels:</p><p>Parallelism:</p><ul><li>Tensor Parallelism</li><li>Data Parallelism</li><li>Model Parallelism</li><li>Pipeline Parallelism</li></ul><p>Inference:</p><ul><li>被 RL、evaluation 所需要</li><li>推理的计算量最后会超过训练</li><li>Prefill 和 Decode<ul><li>Prefill: Compute-bound</li><li>Decode: Memory-bound</li></ul></li><li>加速<ul><li>使用 cheaper model（蒸馏、量化）</li><li>投机解码（Greed Decoding）</li><li>kv cache、batching</li></ul></li></ul><h3 id="3-Scaling-Laws"><a href="#3-Scaling-Laws" class="headerlink" title="3. Scaling Laws"></a>3. Scaling Laws</h3><h3 id="4-Data"><a href="#4-Data" class="headerlink" title="4. Data"></a>4. Data</h3><p>Evaluation:<br>Data Curation:<br>Data processing</p><h3 id="5-Alignment"><a href="#5-Alignment" class="headerlink" title="5. Alignment"></a>5. Alignment</h3><p>base moodel 具有原始潜力，擅长补齐 next token。Alignment 使得模型真正有用。</p><p>Alignmen 的目标</p><ul><li>让 LLM 做到指令跟随</li><li>指定风格<br> -安全性：拒绝回答危险问题</li></ul><p>SFT </p><h3 id="6-Summary"><a href="#6-Summary" class="headerlink" title="6. Summary"></a>6. Summary</h3><p>效率驱动设计决策</p><ul><li><p>计算资源受限的现状</p><ul><li>当前处于计算资源受限的时代</li><li>设计决策需要最大化利用现有硬件资源</li></ul></li><li><p>数据处理优化</p><ul><li>避免在低质量&#x2F;无关数据上浪费计算资源</li><li>确保数据质量，提高计算效率</li></ul></li><li><p>分词策略</p><ul><li>使用原始字节虽然优雅，但在当前模型架构下计算效率低</li><li>需要采用更高效的分词方法</li></ul></li><li><p>模型架构改进</p><ul><li>许多改进都以减少内存使用或计算量（FLOPs）为目标</li><li>例如：共享KV缓存、滑动窗口注意力机制等</li></ul></li><li><p>训练策略</p><ul><li>单轮训练（single epoch）已经足够</li><li>不需要过多轮次的训练</li></ul></li><li><p>扩展法则应用</p><ul><li>在较小的模型上使用较少的计算资源进行超参数调优</li><li>更高效地利用计算资源</li></ul></li><li><p>模型对齐</p><ul><li>如果模型能更好地针对特定用例进行调优</li><li>可以使用更小的基础模型</li></ul></li></ul><h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p>分词器(Tokenizer)是自然语言处理中的基础组件，用于在字符串和整数序列之间进行转换。其主要功能包括：</p><ul><li>将文本字符串编码(encode)为整数序列(tokens)</li><li>将整数序列解码(decode)回原始文本</li><li>建立词汇表(vocabulary)，定义所有可能的token</li></ul><h3 id="2-常见分词方法"><a href="#2-常见分词方法" class="headerlink" title="2. 常见分词方法"></a>2. 常见分词方法</h3><h4 id="2-1-字符级分词-Character-based-Tokenization"><a href="#2-1-字符级分词-Character-based-Tokenization" class="headerlink" title="2.1 字符级分词(Character-based Tokenization)"></a>2.1 字符级分词(Character-based Tokenization)</h4><p><strong>特点：</strong></p><ul><li>将文本分解为单个Unicode字符</li><li>每个字符映射到一个整数(Unicode码点)</li><li>词汇表大小约为150K</li></ul><p><strong>优缺点：</strong></p><ul><li>优点：实现简单，可以处理任何文本</li><li>缺点：<ul><li>词汇表过大</li><li>许多字符使用频率低，效率不高</li><li>序列长度过长</li></ul></li></ul><h4 id="2-2-字节级分词-Byte-based-Tokenization"><a href="#2-2-字节级分词-Byte-based-Tokenization" class="headerlink" title="2.2 字节级分词(Byte-based Tokenization)"></a>2.2 字节级分词(Byte-based Tokenization)</h4><p><strong>特点：</strong></p><ul><li>将文本转换为UTF-8字节序列</li><li>每个字节映射到0-255之间的整数</li><li>词汇表大小固定为256</li></ul><p><strong>优缺点：</strong></p><ul><li>优点：词汇表小，实现简单</li><li>缺点：<ul><li>压缩率低(compression ratio &#x3D; 1)</li><li>序列长度过长</li><li>不适合Transformer等模型(因为注意力机制的计算复杂度与序列长度平方相关)</li></ul></li></ul><h4 id="2-3-词级分词-Word-based-Tokenization"><a href="#2-3-词级分词-Word-based-Tokenization" class="headerlink" title="2.3 词级分词(Word-based Tokenization)"></a>2.3 词级分词(Word-based Tokenization)</h4><p><strong>特点：</strong></p><ul><li>将文本按词分割</li><li>使用正则表达式识别词边界</li><li>词汇表大小取决于训练数据中的唯一词数</li></ul><p><strong>优缺点：</strong></p><ul><li>优点：符合人类直觉，语义单位清晰</li><li>缺点：<ul><li>词汇表可能非常大</li><li>罕见词处理困难</li><li>需要处理未知词(UNK token)</li></ul></li></ul><h4 id="2-4-字节对编码-BPE-Byte-Pair-Encoding"><a href="#2-4-字节对编码-BPE-Byte-Pair-Encoding" class="headerlink" title="2.4 字节对编码(BPE, Byte Pair Encoding)"></a>2.4 字节对编码(BPE, Byte Pair Encoding)</h4><p><strong>特点：</strong></p><ul><li>结合了字节级和词级分词的优点</li><li>通过训练自动确定词汇表</li><li>常用字符序列用单个token表示，罕见序列用多个token表示</li></ul><p><strong>工作原理：</strong></p><ol><li>从字节级token开始</li><li>统计相邻token对的出现频率</li><li>合并最常见的token对</li><li>重复步骤2-3直到达到目标词汇表大小</li></ol><p><strong>优点：</strong></p><ul><li>词汇表大小可控</li><li>压缩效果好</li><li>可以处理未知词</li><li>被GPT-2等主流模型采用</li></ul><h3 id="3-实现示例"><a href="#3-实现示例" class="headerlink" title="3. 实现示例"></a>3. 实现示例</h3><h4 id="3-1-基础接口"><a href="#3-1-基础接口" class="headerlink" title="3.1 基础接口"></a>3.1 基础接口</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Tokenizer</span>(<span class="hljs-title class_ inherited__">ABC</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;分词器抽象接口&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, string: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]:<br>        <span class="hljs-string">&quot;&quot;&quot;将字符串编码为整数序列&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, indices: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;将整数序列解码为字符串&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">raise</span> NotImplementedError<br></code></pre></td></tr></table></figure><h4 id="3-2-BPE分词器参数"><a href="#3-2-BPE分词器参数" class="headerlink" title="3.2 BPE分词器参数"></a>3.2 BPE分词器参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass(<span class="hljs-params">frozen=<span class="hljs-literal">True</span></span>)</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BPETokenizerParams</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;BPE分词器参数&quot;&quot;&quot;</span><br>    vocab: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">bytes</span>]     <span class="hljs-comment"># 索引到字节的映射</span><br>    merges: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>], <span class="hljs-built_in">int</span>]  <span class="hljs-comment"># token对到新token的映射</span><br></code></pre></td></tr></table></figure><h3 id="4-实际应用"><a href="#4-实际应用" class="headerlink" title="4. 实际应用"></a>4. 实际应用</h3><h4 id="4-1-GPT-2分词器"><a href="#4-1-GPT-2分词器" class="headerlink" title="4.1 GPT-2分词器"></a>4.1 GPT-2分词器</h4><ul><li>使用BPE算法</li><li>采用预分词(pre-tokenization)处理</li><li>支持特殊token(如<code>&lt;|endoftext|&gt;</code>)</li><li>可通过tiktoken库使用</li></ul><h4 id="4-2-性能优化方向"><a href="#4-2-性能优化方向" class="headerlink" title="4.2 性能优化方向"></a>4.2 性能优化方向</h4><ol><li>优化merge操作，只处理相关的token对</li><li>实现预分词</li><li>支持特殊token</li><li>提高实现效率</li></ol><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><ul><li>分词是NLP中的必要步骤，但可能不是最优解</li><li>不同分词方法各有优劣，需要根据具体应用场景选择</li><li>BPE是目前最主流的分词方法，在效率和效果上取得了很好的平衡</li><li>未来可能直接使用字节级处理，但目前分词仍然是必要的</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>llm-inference-overview</title>
    <link href="/2025/06/03/llm-inference-overview/"/>
    <url>/2025/06/03/llm-inference-overview/</url>
    
    <content type="html"><![CDATA[<h1 id="一-算法创新-Algorithmic-Innovations"><a href="#一-算法创新-Algorithmic-Innovations" class="headerlink" title="一. 算法创新 (Algorithmic Innovations)"></a>一. 算法创新 (Algorithmic Innovations)</h1><h2 id="1-解码算法-Decoding-Algorithm"><a href="#1-解码算法-Decoding-Algorithm" class="headerlink" title="1. 解码算法 (Decoding Algorithm)"></a>1. 解码算法 (Decoding Algorithm)</h2><h3 id="1-1-非自回归解码-Non-autoregressive-Decoding"><a href="#1-1-非自回归解码-Non-autoregressive-Decoding" class="headerlink" title="1.1 非自回归解码 (Non-autoregressive Decoding)"></a>1.1 非自回归解码 (Non-autoregressive Decoding)</h3><h3 id="1-2-投机采样解码-Speculative-Decoding"><a href="#1-2-投机采样解码-Speculative-Decoding" class="headerlink" title="1.2 投机采样解码 (Speculative Decoding)"></a>1.2 投机采样解码 (Speculative Decoding)</h3><h3 id="1-3-提前退出-Early-Exiting"><a href="#1-3-提前退出-Early-Exiting" class="headerlink" title="1.3 提前退出 (Early Exiting)"></a>1.3 提前退出 (Early Exiting)</h3><h3 id="1-4-级联推理-Cascade-Inference"><a href="#1-4-级联推理-Cascade-Inference" class="headerlink" title="1.4 级联推理 (Cascade Inference)"></a>1.4 级联推理 (Cascade Inference)</h3><h2 id="2-模型结构设计-Model-Architecture-Design"><a href="#2-模型结构设计-Model-Architecture-Design" class="headerlink" title="2. 模型结构设计 (Model Architecture Design)"></a>2. 模型结构设计 (Model Architecture Design)</h2><h3 id="2-1-配置缩减-Config-Downsizing"><a href="#2-1-配置缩减-Config-Downsizing" class="headerlink" title="2.1 配置缩减 (Config Downsizing)"></a>2.1 配置缩减 (Config Downsizing)</h3><h3 id="2-2-注意力简化-Attention-Simplification"><a href="#2-2-注意力简化-Attention-Simplification" class="headerlink" title="2.2 注意力简化 (Attention Simplification)"></a>2.2 注意力简化 (Attention Simplification)</h3><h3 id="2-3-循环单元-Recurrent-Unit"><a href="#2-3-循环单元-Recurrent-Unit" class="headerlink" title="2.3 循环单元 (Recurrent Unit)"></a>2.3 循环单元 (Recurrent Unit)</h3><h3 id="2-4-激活共享-Activation-Sharing"><a href="#2-4-激活共享-Activation-Sharing" class="headerlink" title="2.4 激活共享 (Activation Sharing)"></a>2.4 激活共享 (Activation Sharing)</h3><h3 id="2-5-条件计算-Conditional-Computing"><a href="#2-5-条件计算-Conditional-Computing" class="headerlink" title="2.5 条件计算 (Conditional Computing)"></a>2.5 条件计算 (Conditional Computing)</h3><h2 id="3-模型压缩-Model-Compression"><a href="#3-模型压缩-Model-Compression" class="headerlink" title="3. 模型压缩 (Model Compression)"></a>3. 模型压缩 (Model Compression)</h2><h3 id="3-1-知识蒸馏-Knowledge-Distillation"><a href="#3-1-知识蒸馏-Knowledge-Distillation" class="headerlink" title="3.1 知识蒸馏 (Knowledge Distillation)"></a>3.1 知识蒸馏 (Knowledge Distillation)</h3><h3 id="3-2-网络剪枝-Network-Pruning"><a href="#3-2-网络剪枝-Network-Pruning" class="headerlink" title="3.2 网络剪枝 (Network Pruning)"></a>3.2 网络剪枝 (Network Pruning)</h3><h1 id="二-系统优化-System-Optimizations"><a href="#二-系统优化-System-Optimizations" class="headerlink" title="二. 系统优化 (System Optimizations)"></a>二. 系统优化 (System Optimizations)</h1><h2 id="1-低比特量化-Low-bit-Quantization"><a href="#1-低比特量化-Low-bit-Quantization" class="headerlink" title="1. 低比特量化 (Low-bit Quantization)"></a>1. 低比特量化 (Low-bit Quantization)</h2><h2 id="2-并行计算-Parallel-Computation"><a href="#2-并行计算-Parallel-Computation" class="headerlink" title="2. 并行计算 (Parallel Computation)"></a>2. 并行计算 (Parallel Computation)</h2><h3 id="2-1-模型并行-Model-Parallelism"><a href="#2-1-模型并行-Model-Parallelism" class="headerlink" title="2.1 模型并行 (Model Parallelism)"></a>2.1 模型并行 (Model Parallelism)</h3><h3 id="2-2-去中心化推理-Decentralized-Inference"><a href="#2-2-去中心化推理-Decentralized-Inference" class="headerlink" title="2.2 去中心化推理 (Decentralized Inference)"></a>2.2 去中心化推理 (Decentralized Inference)</h3><h2 id="3-内存管理-Memory-Management"><a href="#3-内存管理-Memory-Management" class="headerlink" title="3. 内存管理 (Memory Management)"></a>3. 内存管理 (Memory Management)</h2><h2 id="4-请求调度-Request-Scheduling"><a href="#4-请求调度-Request-Scheduling" class="headerlink" title="4. 请求调度 (Request Scheduling)"></a>4. 请求调度 (Request Scheduling)</h2><h2 id="5-内核优化-Kernel-Optimizations"><a href="#5-内核优化-Kernel-Optimizations" class="headerlink" title="5. 内核优化 (Kernel Optimizations)"></a>5. 内核优化 (Kernel Optimizations)</h2><h3 id="5-1-算子融合-Operator-Fusion"><a href="#5-1-算子融合-Operator-Fusion" class="headerlink" title="5.1 算子融合 (Operator Fusion)"></a>5.1 算子融合 (Operator Fusion)</h3><h3 id="5-2-定制注意力-Tailored-Attention"><a href="#5-2-定制注意力-Tailored-Attention" class="headerlink" title="5.2 定制注意力 (Tailored Attention)"></a>5.2 定制注意力 (Tailored Attention)</h3><h3 id="5-3-采样优化-Sampling-Optimization"><a href="#5-3-采样优化-Sampling-Optimization" class="headerlink" title="5.3 采样优化 (Sampling Optimization)"></a>5.3 采样优化 (Sampling Optimization)</h3><h3 id="5-4-可变序列长度-Variable-Sequence-Length"><a href="#5-4-可变序列长度-Variable-Sequence-Length" class="headerlink" title="5.4 可变序列长度 (Variable Sequence Length)"></a>5.4 可变序列长度 (Variable Sequence Length)</h3><h3 id="5-5-自动编译-Automatic-Compilation"><a href="#5-5-自动编译-Automatic-Compilation" class="headerlink" title="5.5 自动编译 (Automatic Compilation)"></a>5.5 自动编译 (Automatic Compilation)</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习基础</title>
    <link href="/2025/04/15/basics-of-deep-learning/"/>
    <url>/2025/04/15/basics-of-deep-learning/</url>
    
    <content type="html"><![CDATA[<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>注意力机制是一种让模型能够关注输入序列中最重要部分的机制。就像我们看一张图时，会重点关注图中的某些突出部分，而不是全部，注意力机制就是这样把有限的注意力集中在重点信息上。注意力机制主要由三个关键组件构成：</p><ol><li>Query（查询）：表示当前需要关注的内容</li><li>Key（键）：用于与Query进行匹配，计算相关性</li><li>Value（值）：包含实际的信息内容</li></ol><p>在注意力机制（尤其是 Transformer 的自注意力机制）中，key、value、query 的维度如下：</p><ul><li>假设输入特征维度为 $d_{model}$，则：<ul><li>Query（Q）维度：$d_{model} \times d_k$</li><li>Key（K）维度：$d_{model} \times d_k$</li><li>Value（V）维度：$d_{model} \times d_v$</li></ul></li><li>一般 $d_k &#x3D; d_v &#x3D; d_{model} &#x2F; \text{head数}$（多头注意力时）</li></ul><blockquote><p>注意：</p><ul><li>输入特征维度（$d_{model}$）：这是每个词被表示成向量后的维度，例如768维的向量，意味着每个词可以用768个数字来表示。这些数字共同编码了词的语义、语法等各种特征。</li><li>词表大小（Vocabulary Size）：这是模型能够识别的不同词的数量，例如BERT的词表大小是30,000个词，GPT-3的词表大小是50,000个词。</li><li>举个例子：假设我们有一个词”猫”，在词表中，它可能被分配一个编号，比如1234，但这个编号会被转换成一个768维的向量，比如[0.1, -0.3, 0.5, …]（共768个数字），这个向量包含了”猫”这个词的各种语义信息。</li></ul></blockquote><p>输入 $X$ 通过三个不同的线性变换分别得到 Q、K、V：<br>$$<br>Q &#x3D; XW^Q, \quad K &#x3D; XW^K, \quad V &#x3D; XW^V<br>$$</p><ul><li>$W^Q$ 形状为 $d_{model} \times d_k$</li><li>$W^K$ 形状为 $d_{model} \times d_k$</li><li>$W^V$ 形状为 $d_{model} \times d_v$</li></ul><h3 id="d-model"><a href="#d-model" class="headerlink" title="$d_{model}$"></a>$d_{model}$</h3><ul><li>$d_{model}$ 是输入特征的维度，指输入特征的每个 token 的特征维度。</li><li>在 Transformer 中，$d_{model}$ 通常设置为 768。在 LLM 中，$d_{model}$ 通常设置为 4096，例如llama3.1-70b, llama3.1-405b, d_{model}&#x3D;4096。</li></ul><h3 id="d-k-（Key-Query-的维度）"><a href="#d-k-（Key-Query-的维度）" class="headerlink" title="$d_k$（Key&#x2F;Query 的维度）"></a>$d_k$（Key&#x2F;Query 的维度）</h3><ul><li>$d_k$ 是 Key（K）和 Query（Q）向量的维度。</li><li>在计算注意力分数（即 Q 和 K 的点积）时，Q 和 K 必须有相同的维度 $d_k$。</li><li>$d_k$ 的大小影响注意力分数的分布，通常设置为 $d_{model}&#x2F;\text{head数}$（多头注意力时，每个头分配一部分维度）。</li></ul><h3 id="d-v-（Value-的维度）"><a href="#d-v-（Value-的维度）" class="headerlink" title="$d_v$（Value 的维度）"></a>$d_v$（Value 的维度）</h3><ul><li>$d_v$ 是 Value（V）向量的维度。</li><li>在加权求和得到注意力输出时，输出的每个向量的维度就是 $d_v$。</li><li>$d_v$ 通常也设置为 $d_{model}&#x2F;\text{head数}$，但理论上可以和 $d_k$ 不同。</li></ul><p><strong>总结：</strong></p><ul><li>$d_k$：决定了 Q、K 的维度，影响注意力分数的计算。</li><li>$d_v$：决定了 V 的维度，影响最终注意力输出的特征维度。</li></ul><p>在大语言模型（LLM，如 GPT、BERT 等）中，输入 $X$ 的维度通常为 $(\text{batch_size}, \text{seq_len}, d_{model})$，其中：</p><ul><li>$\text{batch_size}$：批大小，一次输入的样本数量</li><li>$\text{seq_len}$：每个样本的序列长度（如 token 数）</li><li>$d_{model}$：每个 token 的特征维度（如 embedding size）</li></ul><p>例如，若 $\text{batch_size}&#x3D;2$，$\text{seq_len}&#x3D;5$，$d_{model}&#x3D;768$，则 $X$ 的形状为 $(2, 5, 768)$。</p><h3 id="注意力机制的计算过程"><a href="#注意力机制的计算过程" class="headerlink" title="注意力机制的计算过程"></a>注意力机制的计算过程</h3><p>注意力机制的核心计算过程可以分为以下几个步骤：</p><ol><li><p><strong>计算注意力分数（Attention Scores）</strong></p><ul><li>通过 Query 和 Key 的点积计算相关性分数：<br>$$<br>\text{Attention Scores} &#x3D; QK^T<br>$$</li><li>得到的分数矩阵维度为 $(\text{batch_size}, \text{seq_len}, \text{seq_len})$</li><li>每个 batch 中的分数矩阵维度为 $(\text{seq_len}, \text{seq_len})$</li><li>每个分数表示 Query 和 Key 之间的相关性</li></ul></li><li><p><strong>缩放（Scaling）</strong></p><ul><li>为了防止点积结果过大导致 softmax 梯度消失，需要对分数进行缩放：<br>$$<br>\text{Scaled Scores} &#x3D; \frac{QK^T}{\sqrt{d_k}}<br>$$</li><li>除以 $\sqrt{d_k}$ 是为了使方差保持在合理范围内</li></ul></li><li><p><strong>Softmax 归一化</strong></p><ul><li>对缩放后的分数应用 softmax 函数，将分数转换为概率分布：<br>$$<br>\text{Attention Weights} &#x3D; \text{softmax}(\frac{QK^T}{\sqrt{d_k}})<br>$$</li><li>确保所有权重和为 1，且都是非负数</li></ul></li><li><p><strong>加权求和</strong></p><ul><li>使用注意力权重对 Value 进行加权求和：<br>$$<br>\text{Output} &#x3D; \text{Attention Weights} \cdot V<br>$$</li><li>最终输出维度为 $(\text{seq_len}, d_v)$</li></ul></li></ol><h3 id="多头注意力机制（Multi-Head-Attention）"><a href="#多头注意力机制（Multi-Head-Attention）" class="headerlink" title="多头注意力机制（Multi-Head Attention）"></a>多头注意力机制（Multi-Head Attention）</h3><p>多头注意力机制允许模型同时关注不同位置的不同特征：</p><ol><li><p><strong>并行计算</strong></p><ul><li>将输入分成多个头（heads）</li><li>每个头独立计算注意力</li><li>最后将所有头的结果拼接</li></ul></li><li><p><strong>计算过程</strong></p><ul><li>假设有 $h$ 个头，每个头的维度为 $d_k &#x3D; d_v &#x3D; d_{model}&#x2F;h$</li><li>对每个头 $i$：<br>$$<br>\text{head}_i &#x3D; \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)<br>$$</li><li>将所有头的结果拼接：<br>$$<br>\text{MultiHead}(Q, K, V) &#x3D; \text{Concat}(\text{head}_1, …, \text{head}_h)W^O<br>$$</li><li>其中 $W^O$ 是输出投影矩阵</li></ul></li></ol><h3 id="注意力机制的优势"><a href="#注意力机制的优势" class="headerlink" title="注意力机制的优势"></a>注意力机制的优势</h3><ol><li><p><strong>并行计算</strong></p><ul><li>不同于 RNN 需要顺序计算，注意力机制可以并行处理所有位置</li><li>大大提高了计算效率</li></ul></li><li><p><strong>长距离依赖</strong></p><ul><li>可以直接建立任意两个位置之间的关联</li><li>解决了 RNN 难以处理长距离依赖的问题</li></ul></li><li><p><strong>可解释性</strong></p><ul><li>注意力权重可以直观地显示模型关注的重点</li><li>有助于理解模型的决策过程</li></ul></li><li><p><strong>灵活性</strong></p><ul><li>可以处理变长序列</li><li>支持不同类型的输入（文本、图像等）</li></ul></li></ol><h3 id="实际应用示例"><a href="#实际应用示例" class="headerlink" title="实际应用示例"></a>实际应用示例</h3><p>以机器翻译为例，当翻译”我喜欢猫”到英文时：</p><ol><li>Query 代表当前要翻译的词</li><li>Key 和 Value 代表源语言中的每个词</li><li>注意力机制会计算当前词与源语言中每个词的相关性</li><li>根据相关性权重，从源语言中提取最相关的信息</li><li>最终生成对应的英文翻译</li></ol><p>这种机制使得模型能够：</p><ul><li>准确捕捉词与词之间的对应关系</li><li>处理词序不同的语言对</li><li>保持翻译的准确性和流畅性</li></ul><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Transformer 是一种基于注意力机制的神经网络架构，它通过自注意力机制来处理序列数据。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 技术架构与实现深度解析</title>
    <link href="/2025/04/01/vllm-overview/"/>
    <url>/2025/04/01/vllm-overview/</url>
    
    <content type="html"><![CDATA[<h1 id="vLLM-技术架构与实现深度解析"><a href="#vLLM-技术架构与实现深度解析" class="headerlink" title="vLLM 技术架构与实现深度解析"></a>vLLM 技术架构与实现深度解析</h1><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>随着大语言模型（LLM）在各个领域的广泛应用，模型服务部署中的性能瓶颈日益凸显。主要挑战包括推理延迟高、显存利用率低、并发处理能力受限等问题。vLLM 应运而生，作为高性能 LLM 推理引擎，它为这些关键问题提供了创新的解决方案。vLLM 在推理速度提升 2-3 倍的同时，显著降低了显存占用。根据 <a href="https://arxiv.org/abs/2309.06180">vLLM 论文</a> 所声明的，这种性能提升主要源于三个技术创新：</p><ul><li>PagedAttention 技术：把操作系统的内存分页概念应用于 LLM 推理，实现了高效的显存管理机制</li><li>Continuous Batching 处理系统：采用动态请求聚合和智能调度策略，优化了并发请求的处理效率</li><li>分布式计算架构：支持灵活的模型部署方案，适应不同规模的硬件环境和计算需求</li></ul><p>本文将深入探讨 vLLM 的技术架构、核心组件实现以及最佳实践，并将详细分析其性能优化原理，以帮助读者更好地理解 vLLM 的架构概览。</p><!-- ## 2. - **PagedAttention**: 创新的注意力机制计算方案- **连续批处理**: 动态请求合并与调度- **高效内存管理**: 基于分页的 KV Cache 管理- **分布式推理**: 支持模型并行和张量并行- **API 兼容性**: 支持 OpenAI 风格的 API --><h2 id="2-核心特性"><a href="#2-核心特性" class="headerlink" title="2. 核心特性"></a>2. 核心特性</h2><h3 id="2-1-PagedAttention-机制"><a href="#2-1-PagedAttention-机制" class="headerlink" title="2.1 PagedAttention 机制"></a>2.1 PagedAttention 机制</h3><!-- #### 2.1.1 传统 KV Cache 的问题 --><p>PagedAttention 是 vLLM 的核心创新，它通过将操作系统的内存分页思想引入到 LLM 推理过程中，改变了 KV Cache 的管理方式。</p><p>标准的 Transformer 解码过程中，模型需要为每个正在生成的 Sequence 分配一块<strong>连续的显存空间</strong>，用于存储该 Sequence 历史所有 token 的 Key 和 Value（即 KV Cache）。随着推理过程中 Sequence 长度的不断增长，KV Cache 也会线性扩展。如果有多个用户并发请求，每个请求的 Sequence 长度和生命周期都不同，显存中会出现大量”空洞”——即部分已分配但暂未使用或已释放的空间无法被及时复用，造成显存碎片化。下面我将举例说明碎片的产生及其种类：</p><ul><li><p><strong>内部碎片</strong>：在LLM 服务系统中，为了满足未来可能增长的需求，通常为请求预留一个大块连续显存（例如最大 2048 tokens）。但实际上请求可能只用了一部分，例如只用了 300 tokens，这就导致剩下的那部分未被使用的显存浪费了，这种已被分配但未被使用的显存称为内部碎片。</p></li><li><p><strong>外部碎片</strong>：外部碎片是指未被分配出去的小块显存空间零散地分布在整体显存中，无法有效利用，即使总空闲显存量看起来很多，但因为不连续，无法满足新请求对一大块连续内存的需求。例如，请求 1 释放了 500 个 token 的空间，请求 2 释放了 300 个 tokens 的空间，但来了一个需要 1000 个 token 空间的新请求时，尽管总有 800 空闲，但由于它们不连续，仍然无法满足新请求。</p></li></ul><center class="img"><figure>    <img title="碎片化显存示意图" src="/2025/04/01/vllm-overview/frag.png" width="500" height="300">    <figcaption class="image-caption">不同LLM serving system 中碎片化显存示意图</figcaption>  </figure></center><p>为了解决上述显存碎片化严重的问题，提高显存利用率或者说相同显存下支持更多的并发请求，vLLM 引入了 PagedAttention 机制。 PagedAttention 通过引入操作系统的分页机制，将 KV Cache 划分为固定大小的块（页），每个块可独立分配和释放，并通过 Block Table 记录每个 Sequence 的块分配，实现逻辑连续但物理离散的高效显存管理。同时，系统维护 Sequence 到 blocks 的映射关系，利用高效索引结构支持数据的快速定位和动态扩展，从而显著提升了多用户并发场景下的显存利用率和访问效率。另外，在这里需要澄清一个概念，<strong>PagedAttention 并不是一个全新的注意力机制，而是对传统 KV Cache 管理方式的改进</strong>。</p><center class="img"><figure>    <img title="PagedAttention 显存管理示意图" src="/2025/04/01/vllm-overview/blocktable.png" width="650" height="300">    <figcaption class="image-caption">PagedAttention 显存管理示意图</figcaption>  </figure></center><p>此外，vLLM 还支持 KV Cache 共享（KV Cache Sharing），即在多样本生成、beam search 等场景下，多个输出序列可以共享公共部分的 KV Cache，从而显著节省内存，提高并发处理能力。以 beam search 场景为例，给定输入 prompt，模型会首先生成第一个 token 的概率分布。选出概率最高的前 k 个 token（k 即 beam size），每个 token 作为一个“束”（beam）的起点，形成 k 个候选序列，对于每个候选序列，模型再次预测下一个 token 的概率分布,同样选出累计概率最高的前 k 个。重复上述扩展和筛选过程，直到所有 beam 都生成了终止符（如 <eos>），或达到最大长度，最终可以输出 top-k 个序列，供下游任务选择。而这些候选序列的 KV Cache 可以共享公共部分，从而显著节省内存，提高并发处理能力。</eos></p><p>通过这种创新的内存管理方式，PagedAttention 不仅解决了传统 KV Cache 管理的痛点，还带来了显著的性能提升。它的成功实现使得 vLLM 能够在有限的硬件资源下支持更多的并发请求，成为大规模 LLM 服务部署的重要技术基础。</p><h3 id="2-2-调度系统"><a href="#2-2-调度系统" class="headerlink" title="2.2 调度系统"></a>2.2 调度系统</h3><p>vLLM 的调度系统负责管理和优化推理请求：</p><ul><li><strong>请求队列管理</strong>: 动态优先级调度</li><li><strong>批处理优化</strong>: 自适应批大小调整</li><li><strong>资源分配</strong>: GPU 内存和计算资源的动态分配</li></ul><h3 id="2-3-分布式架构"><a href="#2-3-分布式架构" class="headerlink" title="2.3 分布式架构"></a>2.3 分布式架构</h3><p>支持多种并行策略：</p><ul><li><strong>模型并行</strong>: 处理大型模型的跨设备部署</li><li><strong>张量并行</strong>: 提高计算效率</li><li><strong>流水线并行</strong>: 优化延迟和吞吐量</li></ul><h2 id="4-使用方式"><a href="#4-使用方式" class="headerlink" title="4. 使用方式"></a>4. 使用方式</h2><h3 id="4-1-Python-API"><a href="#4-1-Python-API" class="headerlink" title="4.1 Python API"></a>4.1 Python API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> vllm <span class="hljs-keyword">import</span> LLM, SamplingParams<br><br><span class="hljs-comment"># 初始化模型</span><br>llm = LLM(model=<span class="hljs-string">&quot;llama-2-7b&quot;</span>)<br><br><span class="hljs-comment"># 设置生成参数</span><br>sampling_params = SamplingParams(<br>    temperature=<span class="hljs-number">0.8</span>,<br>    top_p=<span class="hljs-number">0.95</span>,<br>    max_tokens=<span class="hljs-number">100</span><br>)<br><br><span class="hljs-comment"># 执行推理</span><br>outputs = llm.generate(<span class="hljs-string">&quot;Tell me a story&quot;</span>, sampling_params)<br></code></pre></td></tr></table></figure><h3 id="4-2-REST-API"><a href="#4-2-REST-API" class="headerlink" title="4.2 REST API"></a>4.2 REST API</h3><p>vLLM 提供了兼容 OpenAI API 的服务接口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl http://localhost:8000/v1/completions \<br>  -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \<br>  -d <span class="hljs-string">&#x27;&#123;</span><br><span class="hljs-string">    &quot;model&quot;: &quot;llama-2-7b&quot;,</span><br><span class="hljs-string">    &quot;prompt&quot;: &quot;Tell me a story&quot;,</span><br><span class="hljs-string">    &quot;max_tokens&quot;: 100</span><br><span class="hljs-string">  &#125;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="5-核心模块功能"><a href="#5-核心模块功能" class="headerlink" title="5. 核心模块功能"></a>5. 核心模块功能</h2><h3 id="5-1-Worker-模块"><a href="#5-1-Worker-模块" class="headerlink" title="5.1 Worker 模块"></a>5.1 Worker 模块</h3><ul><li>负责实际的模型推理执行</li><li>管理计算资源和内存分配</li><li>实现批处理逻辑</li></ul><h3 id="5-2-Scheduler-模块"><a href="#5-2-Scheduler-模块" class="headerlink" title="5.2 Scheduler 模块"></a>5.2 Scheduler 模块</h3><ul><li>请求调度和优先级管理</li><li>动态批处理策略</li><li>负载均衡</li></ul><h3 id="5-3-Cache-Manager-模块"><a href="#5-3-Cache-Manager-模块" class="headerlink" title="5.3 Cache Manager 模块"></a>5.3 Cache Manager 模块</h3><ul><li>KV Cache 的分配和回收</li><li>内存碎片整理</li><li>缓存命中优化</li></ul><h2 id="6-性能优势"><a href="#6-性能优势" class="headerlink" title="6. 性能优势"></a>6. 性能优势</h2><p>与传统推理框架相比，vLLM 具有显著优势：</p><ul><li><strong>更高吞吐量</strong>: 通过批处理优化提升 2-3 倍</li><li><strong>更低延迟</strong>: PagedAttention 减少 40% 响应时间</li><li><strong>更好的内存利用</strong>: 提升 50% 以上的内存效率</li></ul><h2 id="7-最佳实践"><a href="#7-最佳实践" class="headerlink" title="7. 最佳实践"></a>7. 最佳实践</h2><h3 id="7-1-部署建议"><a href="#7-1-部署建议" class="headerlink" title="7.1 部署建议"></a>7.1 部署建议</h3><ul><li>根据负载选择合适的并行策略</li><li>优化批处理参数</li><li>监控内存使用情况</li></ul><h3 id="7-2-性能调优"><a href="#7-2-性能调优" class="headerlink" title="7.2 性能调优"></a>7.2 性能调优</h3><ul><li>调整缓存大小</li><li>优化请求队列配置</li><li>合理设置批处理阈值</li></ul><h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h2><p>vLLM 通过创新的 PagedAttention 机制和高效的调度系统，显著提升了 LLM 推理性能。其核心优势在于：</p><ol><li>高效的内存管理</li><li>灵活的批处理策略</li><li>优秀的系统扩展性</li><li>便捷的接口支持</li></ol><p>这些特性使其成为大规模 LLM 部署的理想选择。随着持续优化和社区贡献，vLLM 的性能和功能还将进一步提升。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>vLLM 官方文档</li><li>PagedAttention 论文</li><li>vLLM GitHub 仓库</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>性能优化</tag>
      
      <tag>推理加速</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>建站初衷</title>
    <link href="/2025/03/25/hello-world/"/>
    <url>/2025/03/25/hello-world/</url>
    
    <content type="html"><![CDATA[<p>现在是2025年3月，我坐在电脑前，想着把创建这个网站的初衷简单地写下来，让r己以后能够时刻回头看看，按照预想的那样去经营它。</p><!-- 起到笔记的作用，防止忘记。 --><p>首先，我希望把它作为一个笔记仓库。作为技术人员，经常会遇到各种挑战，比如项目中踩过的坑、解决过的难题，以及学习新技术时的困惑。如果不把这些经历或知识记录下来，随着时间的推移可能会被遗忘，这很让人觉得可惜。通过写博客能系统地整理和记录这些经验，这样不仅能加深对相关知识的理解，也能在未来遇到类似问题时快速找到解决方案。</p><!-- 记录个人思考，写下来，反复雕琢说服自己。 --><p>此外，我期待在这个网站上多记录自己的想法，并不断完善它们。RL 之父 Sutton 曾建议研究人员：“If you want others to care about what you think, then start by caring yourself. Get a notebook, write your thoughts down, challenge them, and develop them into something worth sharing.”因此，我希望自己能坚持写下去，记录并思考这些想法，通过不断地挑战它们，让它们变得更好。</p><center class="img"><figure>    <img title="RL 之父 Sutton 的建议" src="/2025/03/25/hello-world/sutton.png" width="450" height="450">    <figcaption>RL 之父 Sutton 的建议</figcaption>  </figure></center><!-- 个人展示。 -->同时，我希望把这个网站作为一个展示自我的平台。在这里，我会专注于大模型推理领域，包括但不限于 LLM 推理服务、LLM 推理引擎或框架的开发与优化，分享我在这个领域的认知与见解。通过这些分享，期待与更多志同道合的朋友交流和互动，拓宽自己的视野，并在这个过程中不断提升自己的能力。如果您对我感兴趣，可以通过<a href="mailto:limingliang0527@gmail.com">我的邮箱</a>联系。<br><br><p>最后，我也期待在这个网站上可以不只记录技术上的看法，也能分享生活上的点滴。很多事情，只有记录下来才能更好地被记住；也只有记录下来，才能在回顾的时候体会到当时的感受。在这里分享一篇我很喜欢的散文：</p><blockquote><p><strong>记承天寺夜游</strong> –苏轼<br>元丰六年十月十二日夜，解衣欲睡，月色入户，欣然起行。念无与为乐者，遂至承天寺寻张怀民。怀民亦未寝，相与步于中庭。<br>庭下如积水空明，水中藻荇交横，盖竹柏影也。何夜无月？何处无竹柏？但少闲人如吾两人者耳。</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
