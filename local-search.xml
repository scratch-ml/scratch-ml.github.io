<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>basics-of-deep-learning</title>
    <link href="/2025/04/15/basics-of-deep-learning/"/>
    <url>/2025/04/15/basics-of-deep-learning/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>vLLM 技术架构与实现深度解析</title>
    <link href="/2025/04/01/vllm-overview/"/>
    <url>/2025/04/01/vllm-overview/</url>
    
    <content type="html"><![CDATA[<h1 id="vLLM-技术架构与实现深度解析"><a href="#vLLM-技术架构与实现深度解析" class="headerlink" title="vLLM 技术架构与实现深度解析"></a>vLLM 技术架构与实现深度解析</h1><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>随着大语言模型（LLM）在各个领域的广泛应用，模型服务部署中的性能瓶颈日益凸显。主要挑战包括推理延迟高、显存利用率低、并发处理能力受限等问题。vLLM 应运而生，作为高性能 LLM 推理引擎，它为这些关键问题提供了创新的解决方案。vLLM 在推理速度提升 2-3 倍的同时，显著降低了显存占用。根据 <a href="https://arxiv.org/abs/2309.06180">vLLM 论文</a> 所声明的，这种性能提升主要源于三个技术创新：</p><ul><li>PagedAttention 技术：把操作系统的内存分页概念应用于 LLM 推理，实现了高效的显存管理机制</li><li>连续批处理系统：采用动态请求聚合和智能调度策略，优化了并发请求的处理效率</li><li>分布式计算架构：支持灵活的模型部署方案，适应不同规模的硬件环境和计算需求</li></ul><p>本文将深入探讨 vLLM 的技术架构、核心组件实现以及最佳实践，并将详细分析其性能优化原理，以帮助读者更好地理解 vLLM 的架构概览。</p><!-- ## 2. - **PagedAttention**: 创新的注意力机制计算方案- **连续批处理**: 动态请求合并与调度- **高效内存管理**: 基于分页的 KV Cache 管理- **分布式推理**: 支持模型并行和张量并行- **API 兼容性**: 支持 OpenAI 风格的 API --><h2 id="2-核心特性"><a href="#2-核心特性" class="headerlink" title="2. 核心特性"></a>2. 核心特性</h2><h3 id="2-1-PagedAttention-机制"><a href="#2-1-PagedAttention-机制" class="headerlink" title="2.1 PagedAttention 机制"></a>2.1 PagedAttention 机制</h3><!-- #### 2.1.1 传统 KV Cache 的问题 --><p>PagedAttention 是 vLLM 的核心创新，它通过将操作系统的内存分页思想引入到 LLM 推理过程中，改变了 KV Cache 的管理方式。</p><p>标准的 Transformer 解码过程中，模型需要为每个正在生成的 Sequence 分配一块<strong>连续的显存空间</strong>，用于存储该 Sequence 历史所有 token 的 Key 和 Value（即 KV Cache）。随着推理过程中 Sequence 长度的不断增长，KV Cache 也会线性扩展。如果有多个用户并发请求，每个请求的 Sequence 长度和生命周期都不同，显存中会出现大量”空洞”——即部分已分配但暂未使用或已释放的空间无法被及时复用，造成显存碎片化。下面我将举例说明碎片的产生及其种类：</p><ul><li><p><strong>内部碎片</strong>：在LLM 服务系统中，为了满足未来可能增长的需求，通常为请求预留一个大块连续显存（例如最大 2048 tokens）。但实际上请求可能只用了一部分，例如只用了 300 tokens，这就导致剩下的那部分未被使用的显存浪费了，这种已被分配但未被使用的显存称为内部碎片。</p></li><li><p><strong>外部碎片</strong>：外部碎片是指未被分配出去的小块显存空间零散地分布在整体显存中，无法有效利用，即使总空闲显存量看起来很多，但因为不连续，无法满足新请求对一大块连续内存的需求。例如，请求 1 释放了 500 个 token 的空间，请求 2 释放了 300 个 tokens 的空间，但来了一个需要 1000 个 token 空间的新请求时，尽管总有 800 空闲，但由于它们不连续，仍然无法满足新请求。</p></li></ul><center class="img"><figure>    <img title="碎片化显存示意图" src="/2025/04/01/vllm-overview/frag.png" width="500" height="300">    <figcaption class="image-caption">碎片化显存示意图</figcaption>  </figure></center><p>为了解决上述显存碎片化严重的问题，提高显存利用率或者说相同显存下支持更多的并发请求，vLLM 引入了 PagedAttention 机制。 PagedAttention 通过引入操作系统的分页机制，将 KV Cache 划分为固定大小的块（页），每个块可独立分配和释放，并通过 Block Table 记录每个 Sequence 的块分配，实现逻辑连续但物理离散的高效显存管理。同时，系统维护 Sequence 到 blocks 的映射关系，利用高效索引结构支持数据的快速定位和动态扩展，从而显著提升了多用户并发场景下的显存利用率和访问效率。另外，在这里需要澄清一个概念，<strong>PagedAttention 并不是一个全新的注意力机制，而是对传统 KV Cache 管理方式的改进</strong>。</p><p>关键技术实现</p><ol><li><strong>块分配策略</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 简化的块分配示意代码</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BlockAllocator</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">allocate</span>(<span class="hljs-params">self, num_tokens</span>):<br>        num_blocks = (num_tokens + BLOCK_SIZE - <span class="hljs-number">1</span>) // BLOCK_SIZE<br>        blocks = <span class="hljs-variable language_">self</span>.free_blocks[:num_blocks]<br>        <span class="hljs-variable language_">self</span>.free_blocks = <span class="hljs-variable language_">self</span>.free_blocks[num_blocks:]<br>        <span class="hljs-keyword">return</span> blocks<br></code></pre></td></tr></table></figure></li><li><strong>注意力计算优化</strong><ul><li>将传统的连续注意力计算转换为基于块的计算</li><li>支持跨块的高效访问</li><li>实现了计算和访问的局部性优化</li></ul></li><li><strong>内存回收机制</strong><ul><li>基于引用计数的自动内存回收</li><li>支持细粒度的内存释放</li><li>实现了高效的内存复用</li></ul></li></ol><p>通过这种创新的内存管理方式，PagedAttention 不仅解决了传统 KV Cache 管理的痛点，还带来了显著的性能提升。它的成功实现使得 vLLM 能够在有限的硬件资源下支持更多的并发请求，成为大规模 LLM 服务部署的重要技术基础。</p><h3 id="2-2-调度系统"><a href="#2-2-调度系统" class="headerlink" title="2.2 调度系统"></a>2.2 调度系统</h3><p>vLLM 的调度系统负责管理和优化推理请求：</p><ul><li><strong>请求队列管理</strong>: 动态优先级调度</li><li><strong>批处理优化</strong>: 自适应批大小调整</li><li><strong>资源分配</strong>: GPU 内存和计算资源的动态分配</li></ul><h3 id="2-3-分布式架构"><a href="#2-3-分布式架构" class="headerlink" title="2.3 分布式架构"></a>2.3 分布式架构</h3><p>支持多种并行策略：</p><ul><li><strong>模型并行</strong>: 处理大型模型的跨设备部署</li><li><strong>张量并行</strong>: 提高计算效率</li><li><strong>流水线并行</strong>: 优化延迟和吞吐量</li></ul><h2 id="4-使用方式"><a href="#4-使用方式" class="headerlink" title="4. 使用方式"></a>4. 使用方式</h2><h3 id="4-1-Python-API"><a href="#4-1-Python-API" class="headerlink" title="4.1 Python API"></a>4.1 Python API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> vllm <span class="hljs-keyword">import</span> LLM, SamplingParams<br><br><span class="hljs-comment"># 初始化模型</span><br>llm = LLM(model=<span class="hljs-string">&quot;llama-2-7b&quot;</span>)<br><br><span class="hljs-comment"># 设置生成参数</span><br>sampling_params = SamplingParams(<br>    temperature=<span class="hljs-number">0.8</span>,<br>    top_p=<span class="hljs-number">0.95</span>,<br>    max_tokens=<span class="hljs-number">100</span><br>)<br><br><span class="hljs-comment"># 执行推理</span><br>outputs = llm.generate(<span class="hljs-string">&quot;Tell me a story&quot;</span>, sampling_params)<br></code></pre></td></tr></table></figure><h3 id="4-2-REST-API"><a href="#4-2-REST-API" class="headerlink" title="4.2 REST API"></a>4.2 REST API</h3><p>vLLM 提供了兼容 OpenAI API 的服务接口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl http://localhost:8000/v1/completions \<br>  -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \<br>  -d <span class="hljs-string">&#x27;&#123;</span><br><span class="hljs-string">    &quot;model&quot;: &quot;llama-2-7b&quot;,</span><br><span class="hljs-string">    &quot;prompt&quot;: &quot;Tell me a story&quot;,</span><br><span class="hljs-string">    &quot;max_tokens&quot;: 100</span><br><span class="hljs-string">  &#125;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="5-核心模块功能"><a href="#5-核心模块功能" class="headerlink" title="5. 核心模块功能"></a>5. 核心模块功能</h2><h3 id="5-1-Worker-模块"><a href="#5-1-Worker-模块" class="headerlink" title="5.1 Worker 模块"></a>5.1 Worker 模块</h3><ul><li>负责实际的模型推理执行</li><li>管理计算资源和内存分配</li><li>实现批处理逻辑</li></ul><h3 id="5-2-Scheduler-模块"><a href="#5-2-Scheduler-模块" class="headerlink" title="5.2 Scheduler 模块"></a>5.2 Scheduler 模块</h3><ul><li>请求调度和优先级管理</li><li>动态批处理策略</li><li>负载均衡</li></ul><h3 id="5-3-Cache-Manager-模块"><a href="#5-3-Cache-Manager-模块" class="headerlink" title="5.3 Cache Manager 模块"></a>5.3 Cache Manager 模块</h3><ul><li>KV Cache 的分配和回收</li><li>内存碎片整理</li><li>缓存命中优化</li></ul><h2 id="6-性能优势"><a href="#6-性能优势" class="headerlink" title="6. 性能优势"></a>6. 性能优势</h2><p>与传统推理框架相比，vLLM 具有显著优势：</p><ul><li><strong>更高吞吐量</strong>: 通过批处理优化提升 2-3 倍</li><li><strong>更低延迟</strong>: PagedAttention 减少 40% 响应时间</li><li><strong>更好的内存利用</strong>: 提升 50% 以上的内存效率</li></ul><h2 id="7-最佳实践"><a href="#7-最佳实践" class="headerlink" title="7. 最佳实践"></a>7. 最佳实践</h2><h3 id="7-1-部署建议"><a href="#7-1-部署建议" class="headerlink" title="7.1 部署建议"></a>7.1 部署建议</h3><ul><li>根据负载选择合适的并行策略</li><li>优化批处理参数</li><li>监控内存使用情况</li></ul><h3 id="7-2-性能调优"><a href="#7-2-性能调优" class="headerlink" title="7.2 性能调优"></a>7.2 性能调优</h3><ul><li>调整缓存大小</li><li>优化请求队列配置</li><li>合理设置批处理阈值</li></ul><h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h2><p>vLLM 通过创新的 PagedAttention 机制和高效的调度系统，显著提升了 LLM 推理性能。其核心优势在于：</p><ol><li>高效的内存管理</li><li>灵活的批处理策略</li><li>优秀的系统扩展性</li><li>便捷的接口支持</li></ol><p>这些特性使其成为大规模 LLM 部署的理想选择。随着持续优化和社区贡献，vLLM 的性能和功能还将进一步提升。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>vLLM 官方文档</li><li>PagedAttention 论文</li><li>vLLM GitHub 仓库</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>性能优化</tag>
      
      <tag>推理加速</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>建站初衷</title>
    <link href="/2025/03/25/hello-world/"/>
    <url>/2025/03/25/hello-world/</url>
    
    <content type="html"><![CDATA[<p>现在是2025年3月，我坐在电脑前，想着把创建这个网站的初衷简单地写下来，让r己以后能够时刻回头看看，按照预想的那样去经营它。</p><!-- 起到笔记的作用，防止忘记。 --><p>首先，我希望把它作为一个笔记仓库。作为技术人员，经常会遇到各种挑战，比如项目中踩过的坑、解决过的难题，以及学习新技术时的困惑。如果不把这些经历或知识记录下来，随着时间的推移可能会被遗忘，这很让人觉得可惜。通过写博客能系统地整理和记录这些经验，这样不仅能加深对相关知识的理解，也能在未来遇到类似问题时快速找到解决方案。</p><!-- 记录个人思考，写下来，反复雕琢说服自己。 --><p>此外，我期待在这个网站上多记录自己的想法，并不断完善它们。RL 之父 Sutton 曾建议研究人员：“If you want others to care about what you think, then start by caring yourself. Get a notebook, write your thoughts down, challenge them, and develop them into something worth sharing.”因此，我希望自己能坚持写下去，记录并思考这些想法，通过不断地挑战它们，让它们变得更好。</p><center class="img"><figure>    <img title="RL 之父 Sutton 的建议" src="/2025/03/25/hello-world/sutton.png" width="450" height="450">    <figcaption>RL 之父 Sutton 的建议</figcaption>  </figure></center><!-- 个人展示。 -->同时，我希望把这个网站作为一个展示自我的平台。在这里，我会专注于大模型推理领域，包括但不限于 LLM 推理服务、LLM 推理引擎或框架的开发与优化，分享我在这个领域的认知与见解。通过这些分享，期待与更多志同道合的朋友交流和互动，拓宽自己的视野，并在这个过程中不断提升自己的能力。如果您对我感兴趣，可以通过<a href="mailto:limingliang0527@gmail.com">我的邮箱</a>联系。<br><br><p>最后，我也期待在这个网站上可以不只记录技术上的看法，也能分享生活上的点滴。很多事情，只有记录下来才能更好地被记住；也只有记录下来，才能在回顾的时候体会到当时的感受。在这里分享一篇我很喜欢的散文：</p><blockquote><p><strong>记承天寺夜游</strong> –苏轼<br>元丰六年十月十二日夜，解衣欲睡，月色入户，欣然起行。念无与为乐者，遂至承天寺寻张怀民。怀民亦未寝，相与步于中庭。<br>庭下如积水空明，水中藻荇交横，盖竹柏影也。何夜无月？何处无竹柏？但少闲人如吾两人者耳。</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>随笔</tag>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
